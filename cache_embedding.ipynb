{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a2fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3481a8ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Caching Prompt Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nabla_r2d3_diffsplat_pas import get_diffSlat_model as get_diffSlat_model_pas\n",
    "from peft import LoraConfig, BOFTConfig\n",
    "from peft.utils import get_peft_model_state_dict,load_peft_weights\n",
    "from extensions.diffusers_diffsplat import PixArtSigmaMVPipeline\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "import einops\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb439d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging    \n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_dict = get_diffSlat_model_pas(logger,load_text_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d784a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline: PixArtSigmaMVPipeline = model_dict[\"pipeline\"]\n",
    "device = \"cuda:0\"\n",
    "pipeline = pipeline.to(device)\n",
    "# pipeline = pipeline.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64df97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.1 Cache positive prompt Embedding( GObj-83k prompt dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513c46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = 'output/Cached_embedding/pas_embedding_G-obj-83k'\n",
    "\n",
    "file_path = 'src/nablagflow/alignment/assets/G-obj-83k.txt'\n",
    "### output embedding cache path\n",
    "### input prompt path \n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "prompts = [line.strip() for line in lines]  # Remove any leading/trailing whitespace characters\n",
    "\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7108b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp  {file_path} {save_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7fdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing 100-200:   0%|          | 100/83296 [00:40<9:21:15,  2.47it/s]\n",
      "processing 24300-24400:  29%|██▉       | 24300/83296 [57:04<2:15:27,  7.26it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for prompt in tqdm(prompts):\n",
    "idx = 0\n",
    "batch_size = 100\n",
    "# pbar = tqdm( len(prompts)//batch_size)\n",
    "pbar = tqdm(range(0, len(prompts)))\n",
    "# total=100\n",
    "# save_idx = 0\n",
    "cache_every = 1000\n",
    "\n",
    "\n",
    "prompt_embeds_list = []\n",
    "prompt_attention_masks_list = []\n",
    "negative_prompt_attention_mask =None\n",
    "negative_prompt_embeds = None\n",
    "prompts_list=[]\n",
    "\n",
    "half_precision=True \n",
    "with torch.autocast(\"cuda\", torch.bfloat16 if half_precision else torch.float32):\n",
    "    with torch.inference_mode():\n",
    "        reach_end = False\n",
    "        \n",
    "        while True:\n",
    "            upper_bound = idx + batch_size\n",
    "            if upper_bound >= len(prompts):\n",
    "                upper_bound = len(prompts)\n",
    "                reach_end = True\n",
    "            if idx > len(prompts):\n",
    "                break\n",
    "            pbar.set_description(f\"processing {idx}-{upper_bound}\")\n",
    "            prompt = prompts[idx:upper_bound]\n",
    "            prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = pipeline.encode_prompt(\n",
    "                prompt=prompt,\n",
    "                do_classifier_free_guidance= True,\n",
    "                # negative_prompt= \"\",\n",
    "                device= device,\n",
    "                clean_caption=True,\n",
    "            )\n",
    "            prompt_embeds_list.append(prompt_embeds.cpu())\n",
    "            prompt_attention_masks_list.append(prompt_attention_mask.cpu())\n",
    "            \n",
    "            idx += batch_size\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "            prompts_list.append(prompt)\n",
    "            \n",
    "            if (idx) %cache_every == 0 or reach_end:\n",
    "                prompt_embed_all = torch.cat(prompt_embeds_list,dim=0)\n",
    "                prompt_attention_masks_all = torch.cat(prompt_attention_masks_list,dim=0)\n",
    "                a =einops.rearrange(prompt_embed_all, 'b n d -> (b n) d')\n",
    "                b =einops.rearrange(prompt_attention_masks_all, 'b n -> (b n)')\n",
    "                selected = a[b==1]\n",
    "                name = f'{idx-cache_every +1:05d}-{idx:05d}'\n",
    "                if reach_end:\n",
    "                    final_start = len(prompts) - len(prompts)%cache_every +1\n",
    "                    name = f'{final_start:05d}-{upper_bound:05d}'\n",
    "                torch.save(selected, os.path.join(save_dir,f'./{name}prompt_embeds.pt'))\n",
    "                torch.save(b,os.path.join(save_dir,f\"./{name}prompt_attention_masks.pt\"))\n",
    "                # torch.save(prompts_list, './prompts_list.pt')\n",
    "                # print(\"Saved\")\n",
    "                pbar.set_description(f\"Saved {name}\")\n",
    "                prompt_embeds_list = []\n",
    "                prompt_attention_masks_list = []\n",
    "                torch.cuda.empty_cache()\n",
    "            if reach_end:\n",
    "                break\n",
    "                \n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae01546",
   "metadata": {},
   "source": [
    "### 1.2 Cache Negative Prompt Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dedfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    with torch.autocast(\"cuda\", torch.bfloat16 if half_precision else torch.float32):\n",
    "        prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = pipeline.encode_prompt(\n",
    "            prompt='',\n",
    "            do_classifier_free_guidance= True,\n",
    "            # negative_prompt= \"\",\n",
    "            device= \"cuda\",\n",
    "            clean_caption=True,\n",
    "        )\n",
    "    torch.save(prompt_embeds, os.path.join(save_dir,'negative_prompt_embeds.pt'))\n",
    "    torch.save(prompt_attention_mask, os.path.join(save_dir,'negative_prompt_attention_mask.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89f3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5ae4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NablaR2D3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
